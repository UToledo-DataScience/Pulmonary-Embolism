{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Acknowledgements\nhttps://www.kaggle.com/allunia/pulmonary-dicom-preprocessing - DICOM preprocessing\n\nhttps://www.kaggle.com/seraphwedd18/pe-detection-with-keras-model-creation - DICOM preprocessing\n\nhttps://www.kaggle.com/redwankarimsony/rsna-str-pe-gradient-sigmoid-windowing/comments - DICOM windowing"},{"metadata":{},"cell_type":"markdown","source":"# **IMPORTING DATA**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport pydicom\nimport vtk\nimport cv2\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\nimport pylab as pl\nimport numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom time import time\nfrom random import shuffle, sample, shuffle, randrange\nfrom vtk.util import numpy_support","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_ROOT = Path(\"../input/rsna-str-pulmonary-embolism-detection\")\nTRAIN_ROOT = DATA_ROOT/\"train\"\nTEST_ROOT = DATA_ROOT/'test'\n\nIMAGE_RESOLUTION = (256, 256)\n\ntrain_csv = pd.read_csv(DATA_ROOT/\"train.csv\")","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reader = vtk.vtkDICOMImageReader()\ndef get_img(path):\n    reader.SetFileName(path)\n    reader.Update()\n    _extent = reader.GetDataExtent()\n    ConstPixelDims = [_extent[1]-_extent[0]+1, _extent[3]-_extent[2]+1, _extent[5]-_extent[4]+1]\n\n    ConstPixelSpacing = reader.GetPixelSpacing()\n    \n    dcm_fields = [reader.GetRescaleSlope(), reader.GetRescaleOffset()]\n    \n    imageData = reader.GetOutput()\n    pointData = imageData.GetPointData()\n    arrayData = pointData.GetArray(0)\n    ArrayDicom = numpy_support.vtk_to_numpy(arrayData)\n    ArrayDicom = ArrayDicom.reshape(ConstPixelDims, order='F')\n    ArrayDicom = cv2.resize(ArrayDicom, IMAGE_RESOLUTION)\n    return ArrayDicom, dcm_fields","execution_count":33,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Processing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing the image\n# Windowing rescales the image to highlight different parts of the image\n\ndef lung_window(img, dcm_fields):\n    width = 1600\n    length = -600\n    window_min = length - (width/2)\n    window_max = length + (width/2)\n    slope, intercept = dcm_fields\n    #img += np.abs(np.min(img))\n    img = img * slope + intercept\n    img[img < window_min] = window_min\n    img[img > window_max] = window_max\n    img = (img - np.min(img)) / (np.max(img) - np.min(img))\n    #print(np.min(img), np.max(img))\n    return img\n\ndef map_to_gradient(grey_img):\n    rainbow_img = np.zeros((grey_img.shape[0], grey_img.shape[1], 3))\n    rainbow_img[:, :, 0] = np.clip(4 * grey_img - 2, 0, 1.0) * (grey_img > 0) * (grey_img <= 1.0)\n    rainbow_img[:, :, 1] =  np.clip(4 * grey_img * (grey_img <=0.75), 0,1) + np.clip((-4*grey_img + 4) * (grey_img > 0.75), 0, 1)\n    rainbow_img[:, :, 2] = np.clip(-4 * grey_img + 2, 0, 1.0) * (grey_img > 0) * (grey_img <= 1.0)\n    return rainbow_img\n\ndef rainbow_window(img, dcm_fields):\n    grey_img = lung_window(img, dcm_fields)\n    return map_to_gradient(grey_img)\n\ndef all_channels_window(img, dcm_fields):\n    grey_img = lung_window(img, dcm_fields) * 3.0\n    all_chan_img = np.zeros((grey_img.shape[0], grey_img.shape[1], 3))\n    all_chan_img[:, :, 2] = np.clip(grey_img, 0.0, 1.0)\n    all_chan_img[:, :, 0] = np.clip(grey_img - 1.0, 0.0, 1.0)\n    all_chan_img[:, :, 1] = np.clip(grey_img - 2.0, 0.0, 1.0)\n    return all_chan_img","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Data loading functions\n\nstudies = os.listdir(TRAIN_ROOT)\n#studies = studies[:len(studies)//16]\n\nfunc = lambda x: int((2**15 + x)*(255/2**16))\nint16_to_uint8 = np.vectorize(func)\n\ndef load_scans(dcm_path):\n    # otherwise we sort by ImagePositionPatient (z-coordinate) or by SliceLocation\n    slices = []\n    fields = []\n    for file in os.listdir(dcm_path):\n        image, dcm_fields = get_img(dcm_path + \"/\" + file)\n        #image = rainbow_window(image, dcm_fields)\n        slices.append(image)\n        fields.append(dcm_fields)\n\n    return slices, fields\n\ndef filter_scanner(raw_pixelarrays):\n    # in OSIC we find outside-scanner-regions with raw-values of -2000. \n    # Let's threshold between air (0) and this default (-2000) using -1000\n    raw_pixelarrays[raw_pixelarrays <= -1000] = -1000\n    return raw_pixelarrays\n\nindex = 125\n\ndef load_scans_from_study(study):\n    scans = []\n    fields = []\n    series = []\n    for s in os.listdir(TRAIN_ROOT/study):\n        series.append(s)\n        scan_set, dcm_fields = load_scans(str(TRAIN_ROOT/study/s))\n        scans.append(scan_set)\n        fields.append(dcm_fields)\n        \n    return series, scans, fields\n\ndef load_individual_scan(scan_path):\n    scan, fields = get_img(scan_path)\n    scan = rainbow_window(scan, fields)\n    return scan\n\ndef load_batch_scans(scan_paths):\n    scans = np.zeros((len(scan_paths), IMAGE_RESOLUTION[0], IMAGE_RESOLUTION[1], 3))\n    for i, path in enumerate(scan_paths):\n        s, f = get_img(path)\n        s = rainbow_window(s, f)\n        scans[i] = s\n        \n    return scans","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# t0 = time()\n# scans, fields = load_scans(str(TRAIN_ROOT/studies[100]/os.listdir(TRAIN_ROOT/studies[100])[0]))\n# t1 = time()\n# print(t1 - t0)\n# s = scans[index]\n# #print(s)\n# pl.imshow(s, cmap=pl.cm.bone)\n# pl.show()\n\n\n# t0 = time()\n# s = rainbow_window(scans[index], fields[index])\n# t1 = time()\n# print(t1 - t0)\n# #print(s)\n# pl.imshow(s, cmap=pl.cm.bone)\n# pl.show()","execution_count":36,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **TFRecords**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TFRecord formats\n\nUNLABELED_TFRECORD_FORMAT = {'SpecificCharacterSet': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'ImageType': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SOPClassUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SOPInstanceUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'Modality': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SliceThickness': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'KVP': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'GantryDetectorTilt': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'TableHeight': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'RotationDirection': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'XRayTubeCurrent': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'Exposure': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'ConvolutionKernel': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'PatientPosition': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'StudyInstanceUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SeriesInstanceUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SeriesNumber': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'InstanceNumber': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'ImagePositionPatient': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'ImageOrientationPatient': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'FrameOfReferenceUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SamplesPerPixel': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'PhotometricInterpretation': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'Rows': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'Columns': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'PixelSpacing': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'BitsAllocated': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'BitsStored': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'HighBit': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'PixelRepresentation': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'WindowCenter': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'WindowWidth': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'RescaleIntercept': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'RescaleSlope': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'image': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None)}\n\nLABELED_TFRECORD_FORMAT = {'SpecificCharacterSet': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'ImageType': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SOPClassUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SOPInstanceUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'Modality': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SliceThickness': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'KVP': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'GantryDetectorTilt': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'TableHeight': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'RotationDirection': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'XRayTubeCurrent': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'Exposure': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'ConvolutionKernel': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'PatientPosition': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'StudyInstanceUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SeriesInstanceUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SeriesNumber': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'InstanceNumber': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'ImagePositionPatient': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'ImageOrientationPatient': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'FrameOfReferenceUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SamplesPerPixel': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'PhotometricInterpretation': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'Rows': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'Columns': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'PixelSpacing': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'BitsAllocated': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'BitsStored': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'HighBit': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'PixelRepresentation': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'WindowCenter': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'WindowWidth': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'RescaleIntercept': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'RescaleSlope': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'negative_exam_for_pe': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'qa_motion': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'qa_contrast': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'flow_artifact': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'rv_lv_ratio_gte_1': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'rv_lv_ratio_lt_1': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'leftsided_pe': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'chronic_pe': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'true_filling_defect_not_pe': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'rightsided_pe': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'acute_and_chronic_pe': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'central_pe': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'indeterminate': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'pe_present_on_image': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'image': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None)}","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TFRecord data loaders\n\ndef read_labeled_tfrecord(example):\n    return read_tfrecord(example, LABELED_TFRECORD_FORMAT)\n\ndef read_unlabeled_tfrecord(example):\n    return read_tfrecord(example, UNLABELED_TFRECORD_FORMAT)\n\ndef read_tfrecord(example, record_format):\n    try:\n        example = tf.io.parse_single_example(example, record_format)\n    except:\n        print (example)\n        raise\n    \n    data = {k:tf.cast(example[k], record_format[k].dtype) for k in example}\n        \n    return data\n\ndef load_dataset(filenames, batch_size, repeat=True, labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord).batch(batch_size, drop_remainder=True)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset.repeat() if repeat else dataset","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing function to transform the raw numpy images\n# to scaled, windowed, images.\n\ndef preprocess(data):\n    image = np.array([np.frombuffer(im.numpy(), dtype=np.int16) for im in data['image']]).reshape((batch_size, 512, 512, 1))\n    image = tf.image.resize(image, (IMAGE_RESOLUTION[0], IMAGE_RESOLUTION[1])).numpy()\n    image = np.squeeze(image, -1)\n    \n    out_images = np.zeros((batch_size, IMAGE_RESOLUTION[0], IMAGE_RESOLUTION[1], 3))\n    \n    for i, im in enumerate(image):\n        dcm_fields = (int(float(data['RescaleSlope'].numpy()[i].decode())), int(float(data['RescaleIntercept'].numpy()[i].decode())))\n        out_images[i] = rainbow_window(image[i], dcm_fields)\n\n    label = data['pe_present_on_image'].numpy()\n    label = tf.reshape(label, (batch_size, 1))\n    \n    return out_images, label","execution_count":39,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Splitting Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_IMAGES = 500000\n\nimage_labels = train_csv[['StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID', 'pe_present_on_image']]\n\nexam_labels = train_csv[['StudyInstanceUID', 'negative_exam_for_pe', 'rv_lv_ratio_gte_1',\n                         'rv_lv_ratio_lt_1', 'leftsided_pe', 'chronic_pe', 'rightsided_pe',\n                         'acute_and_chronic_pe', 'central_pe', 'indeterminate']].drop_duplicates('StudyInstanceUID')","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_indices = list(image_labels.loc[image_labels['pe_present_on_image'] == 0].axes[0])\npositive_indices = list(image_labels.loc[image_labels['pe_present_on_image'] == 1].axes[0])\n\nall_indices = negative_indices + positive_indices","execution_count":41,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 24\nepochs = 2\nlearning_rate = 1e-6\nimage_opt = keras.optimizers.Adam(learning_rate=learning_rate, amsgrad=True)","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model definitions\n\ndef get_model():\n    image_inp = keras.Input(shape=(IMAGE_RESOLUTION[0], IMAGE_RESOLUTION[1], 3))\n    image_model = keras.applications.EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')(image_inp)\n    image_model = layers.Dense(1, activation='sigmoid')(image_model)\n    image_model = keras.Model(inputs=image_inp, outputs=image_model)\n\n    image_model.summary()\n    \n    return image_model","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loss weights to apply to the calculated loss\n# Ratio of all the training samples to the positive training samples\nloss_weights = np.full((batch_size, 1), len(all_indices)/len(positive_indices))\n\n# Loss function with the option of weighting classes\n# Uses the defined loss_weights above\ndef loss_function(labels, logits, weighted=True):\n    loss = keras.losses.binary_crossentropy(labels, logits, from_logits=True, label_smoothing=0.05)\n    \n    if weighted:\n        weights = tf.cast(tf.math.greater(labels, 0), tf.float32)*loss_weights\n        weights += tf.cast(tf.math.equal(weights, 0), tf.float32)\n        weights = tf.reshape(weights, (weights.shape[0],))\n        \n        loss = tf.math.multiply(loss, weights)\n\n    loss = tf.math.reduce_mean(loss)\n    \n    return loss\n\n#@tf.function\ndef image_train_step(image_model, image, labels):\n    with tf.GradientTape() as tape:\n        logits = image_model(image)\n        loss = loss_function(labels, logits, weighted=True)\n        \n    gradients = tape.gradient(loss, image_model.trainable_variables)\n    image_opt.apply_gradients(zip(gradients, image_model.trainable_variables))\n    \n    return loss, logits\n\n#@tf.function\ndef image_eval_step(image_model, image, labels):\n    logits = image_model(image)\n    loss = loss_function(labels, logits, weighted=True)\n    \n    return loss, logits\n\ntrain_root = \"../input/rsna-pe-tfrecords-v2/train/\"\nall_tfrecords = [train_root+record for record in os.listdir(train_root)]","execution_count":44,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# doing k-fold cross validation from the full training dataset to preserve the skewed ratio\nk = 5\nfold_size = 50000\nimages_per_record = 559\n\n# generating folds\ndef generate_folds(k, fold_size, tfrecords):\n    assert fold_size < len(tfrecords) * images_per_record / k\n    \n    # fold_size can be set to -1 to use all TFRecords\n    if fold_size == -1:\n        records_per_fold = len(tfrecords) // k\n    else:\n        records_per_fold = fold_size // images_per_record\n\n    folds = list()\n    for i in range(k):\n        shuffle(tfrecords)\n\n        fold = tfrecords[:records_per_fold]\n        folds.append(fold)\n\n        tfrecords = tfrecords[records_per_fold:]\n    \n    return folds\n    \n# running CV\ndef cross_validation_training(folds):\n    loss_met = keras.metrics.Mean()\n    auc_pr_met = keras.metrics.BinaryAccuracy()\n\n    count = 0\n\n    validation_history = []\n    for i in range(len(folds)):\n        keras.backend.clear_session()\n        \n        image_model = get_model()\n        \n        prev = folds[:i]\n        post = folds[i+1:] if i < len(folds)-1 else None\n        train_folds = prev + post if post is not None else prev\n        train_dataset = []\n        # there's probably a better way of doing this\n        for f in train_folds:\n            for record in f:\n                train_dataset.append(record)\n\n        train_dataset = load_dataset(train_dataset, batch_size, repeat=False)\n\n        validation_dataset = folds[i]\n        validation_dataset = load_dataset(validation_dataset, batch_size, repeat=False)\n\n        for e in range(epochs):\n            for n, data in enumerate(train_dataset):\n                # preprocessing before feeding to model\n                image, label = preprocess(data)\n\n                # feed-forward and backprop\n                loss, logits = image_train_step(image_model, image, label)\n\n                loss_met(loss)\n                auc_pr_met(label, logits)\n\n                print(f\"Epoch {e} of {epochs}, training data point {n+1} average loss, accuracy: {loss_met.result()}, {auc_pr_met.result()}    \\r\", end='')\n\n            print(f\"Epoch {e} of {epochs}, fold {i+1} training average loss, accuracy: {loss_met.result()}, {auc_pr_met.result()}\")\n            \n        print(f\"Fold {i+1} training average loss, accuracy: {loss_met.result()}, {auc_pr_met.result()}\")\n        print(\"------------------\")\n\n        loss_met.reset_states()\n        auc_pr_met.reset_states()\n        for n, data in enumerate(validation_dataset):\n            # preprocessing before feeding to model\n            image, label = preprocess(data)\n\n            # feed-forward and backprop\n            loss, logits = image_eval_step(image_model, image, label)\n\n            loss_met(loss)\n            auc_pr_met(label, logits)\n\n            print(f\"Validation data point {n+1} average loss, accuracy: {loss_met.result()}, {auc_pr_met.result()}    \\r\", end='')\n            \n            \n        validation_history.append(loss_met.result())\n        print(f\"Fold {i+1} validation average loss, accuracy: {loss_met.result()}, {auc_pr_met.result()}               \")\n        print(\"------------------\")\n        \n        image_model.save_weights(f\"image_weights_{i+1}.h5\")\n        \n        del image_model\n        \n    return validation_history\n\nfolds = generate_folds(k, fold_size, all_tfrecords)\nvalidation_history = cross_validation_training(folds)\n\nfor i, validation in enumerate(validation_history):\n    print(f\"Fold {i+1} validation: {validation}\")","execution_count":45,"outputs":[{"output_type":"stream","text":"Model: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n_________________________________________________________________\nefficientnetb0 (Functional)  (None, 1280)              4049571   \n_________________________________________________________________\ndense (Dense)                (None, 1)                 1281      \n=================================================================\nTotal params: 4,050,852\nTrainable params: 4,008,829\nNon-trainable params: 42,023\n_________________________________________________________________\nEpoch 0 of 2, fold 1 training average loss, AUC: 1.6315443515777588, 0.63844084739685063968506     \nEpoch 1 of 2, fold 1 training average loss, AUC: 1.6289920806884766, 0.77822577953338625333862    \nFold 1 training average loss, AUC: 1.6289920806884766, 0.7782257795333862\n------------------\nFold 1 validation average loss, AUC: 1.4268226623535156, 0.94202899932861333286133    \n------------------\nModel: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n_________________________________________________________________\nefficientnetb0 (Functional)  (None, 1280)              4049571   \n_________________________________________________________________\ndense (Dense)                (None, 1)                 1281      \n=================================================================\nTotal params: 4,050,852\nTrainable params: 4,008,829\nNon-trainable params: 42,023\n_________________________________________________________________\nEpoch 0 of 2, fold 2 training average loss, AUC: 1.7096925973892212, 0.91056036949157714915771    \nEpoch 1 of 2, fold 2 training average loss, AUC: 1.7340795993804932, 0.90709728002548220254822    \nFold 2 training average loss, AUC: 1.7340795993804932, 0.9070972800254822\n------------------\nFold 2 validation average loss, AUC: 0.910746693611145, 1.045, 1.0     \n------------------\nModel: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n_________________________________________________________________\nefficientnetb0 (Functional)  (None, 1280)              4049571   \n_________________________________________________________________\ndense (Dense)                (None, 1)                 1281      \n=================================================================\nTotal params: 4,050,852\nTrainable params: 4,008,829\nNon-trainable params: 42,023\n_________________________________________________________________\nEpoch 0 of 2, fold 3 training average loss, AUC: 1.602738380432129, 0.84446841478347787834778     \nEpoch 1 of 2, fold 3 training average loss, AUC: 1.6712602376937866, 0.84170651435852053585205    \nFold 3 training average loss, AUC: 1.6712602376937866, 0.8417065143585205\n------------------\nFold 3 validation average loss, AUC: 0.9649171233177185, 0.085, 0.0    \n------------------\nModel: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n_________________________________________________________________\nefficientnetb0 (Functional)  (None, 1280)              4049571   \n_________________________________________________________________\ndense (Dense)                (None, 1)                 1281      \n=================================================================\nTotal params: 4,050,852\nTrainable params: 4,008,829\nNon-trainable params: 42,023\n_________________________________________________________________\nEpoch 0 of 2, fold 4 training average loss, AUC: 1.2068864107131958, 0.76508623361587526158752     \nEpoch 1 of 2, fold 4 training average loss, AUC: 1.2252132892608643, 0.85207337141036994103699    \nFold 4 training average loss, AUC: 1.2252132892608643, 0.8520733714103699\n------------------\nFold 4 validation average loss, AUC: 3.0086898803710938, 0.76630437374114997411499    \n------------------\nModel: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n_________________________________________________________________\nefficientnetb0 (Functional)  (None, 1280)              4049571   \n_________________________________________________________________\ndense (Dense)                (None, 1)                 1281      \n=================================================================\nTotal params: 4,050,852\nTrainable params: 4,008,829\nNon-trainable params: 42,023\n_________________________________________________________________\nEpoch 0 of 2, fold 5 training average loss, AUC: 1.880879521369934, 0.89511495828628542862854     \nEpoch 1 of 2, fold 5 training average loss, AUC: 1.738377332687378, 0.90929025411605831160583     \nFold 5 training average loss, AUC: 1.738377332687378, 0.9092902541160583\n------------------\nFold 5 validation average loss, AUC: 1.791927456855774, 0.90217393636703493670349     \n------------------\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}