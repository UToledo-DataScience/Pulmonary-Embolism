{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Acknowledgements\nhttps://www.kaggle.com/allunia/pulmonary-dicom-preprocessing - DICOM preprocessing\n\nhttps://www.kaggle.com/seraphwedd18/pe-detection-with-keras-model-creation - DICOM preprocessing\n\nhttps://www.kaggle.com/redwankarimsony/rsna-str-pe-gradient-sigmoid-windowing/comments - DICOM windowing"},{"metadata":{},"cell_type":"markdown","source":"# **IMPORTING DATA**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport pydicom\nimport vtk\nimport cv2\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\nimport pylab as pl\nimport numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom time import time\nfrom random import shuffle, sample, shuffle, randrange\nfrom vtk.util import numpy_support","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_ROOT = Path(\"../input/rsna-str-pulmonary-embolism-detection\")\nTRAIN_ROOT = DATA_ROOT/\"train\"\nTEST_ROOT = DATA_ROOT/'test'\n\nIMAGE_RESOLUTION = (256, 256)\n\ntrain_csv = pd.read_csv(DATA_ROOT/\"train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reader = vtk.vtkDICOMImageReader()\ndef get_img(path):\n    reader.SetFileName(path)\n    reader.Update()\n    _extent = reader.GetDataExtent()\n    ConstPixelDims = [_extent[1]-_extent[0]+1, _extent[3]-_extent[2]+1, _extent[5]-_extent[4]+1]\n\n    ConstPixelSpacing = reader.GetPixelSpacing()\n    \n    dcm_fields = [reader.GetRescaleSlope(), reader.GetRescaleOffset()]\n    \n    imageData = reader.GetOutput()\n    pointData = imageData.GetPointData()\n    arrayData = pointData.GetArray(0)\n    ArrayDicom = numpy_support.vtk_to_numpy(arrayData)\n    ArrayDicom = ArrayDicom.reshape(ConstPixelDims, order='F')\n    ArrayDicom = cv2.resize(ArrayDicom, IMAGE_RESOLUTION)\n    return ArrayDicom, dcm_fields","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Processing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing the image\n# Windowing rescales the image to highlight different parts of the image\n\ndef lung_window(img, dcm_fields):\n    width = 1600\n    length = -600\n    window_min = length - (width/2)\n    window_max = length + (width/2)\n    slope, intercept = dcm_fields\n    #img += np.abs(np.min(img))\n    img = img * slope + intercept\n    img[img < window_min] = window_min\n    img[img > window_max] = window_max\n    img = (img - np.min(img)) / (np.max(img) - np.min(img))\n    #print(np.min(img), np.max(img))\n    return img\n\ndef map_to_gradient(grey_img):\n    rainbow_img = np.zeros((grey_img.shape[0], grey_img.shape[1], 3))\n    rainbow_img[:, :, 0] = np.clip(4 * grey_img - 2, 0, 1.0) * (grey_img > 0) * (grey_img <= 1.0)\n    rainbow_img[:, :, 1] =  np.clip(4 * grey_img * (grey_img <=0.75), 0,1) + np.clip((-4*grey_img + 4) * (grey_img > 0.75), 0, 1)\n    rainbow_img[:, :, 2] = np.clip(-4 * grey_img + 2, 0, 1.0) * (grey_img > 0) * (grey_img <= 1.0)\n    return rainbow_img\n\ndef rainbow_window(img, dcm_fields):\n    grey_img = lung_window(img, dcm_fields)\n    return map_to_gradient(grey_img)\n\ndef all_channels_window(img, dcm_fields):\n    grey_img = lung_window(img, dcm_fields) * 3.0\n    all_chan_img = np.zeros((grey_img.shape[0], grey_img.shape[1], 3))\n    all_chan_img[:, :, 2] = np.clip(grey_img, 0.0, 1.0)\n    all_chan_img[:, :, 0] = np.clip(grey_img - 1.0, 0.0, 1.0)\n    all_chan_img[:, :, 1] = np.clip(grey_img - 2.0, 0.0, 1.0)\n    return all_chan_img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Data loading functions\n\nstudies = os.listdir(TRAIN_ROOT)\n#studies = studies[:len(studies)//16]\n\nfunc = lambda x: int((2**15 + x)*(255/2**16))\nint16_to_uint8 = np.vectorize(func)\n\ndef load_scans(dcm_path):\n    # otherwise we sort by ImagePositionPatient (z-coordinate) or by SliceLocation\n    slices = []\n    fields = []\n    for file in os.listdir(dcm_path):\n        image, dcm_fields = get_img(dcm_path + \"/\" + file)\n        #image = rainbow_window(image, dcm_fields)\n        slices.append(image)\n        fields.append(dcm_fields)\n\n    return slices, fields\n\ndef filter_scanner(raw_pixelarrays):\n    # in OSIC we find outside-scanner-regions with raw-values of -2000. \n    # Let's threshold between air (0) and this default (-2000) using -1000\n    raw_pixelarrays[raw_pixelarrays <= -1000] = -1000\n    return raw_pixelarrays\n\nindex = 125\n\ndef load_scans_from_study(study):\n    scans = []\n    fields = []\n    series = []\n    for s in os.listdir(TRAIN_ROOT/study):\n        series.append(s)\n        scan_set, dcm_fields = load_scans(str(TRAIN_ROOT/study/s))\n        scans.append(scan_set)\n        fields.append(dcm_fields)\n        \n    return series, scans, fields\n\ndef load_individual_scan(scan_path):\n    scan, fields = get_img(scan_path)\n    scan = rainbow_window(scan, fields)\n    return scan\n\ndef load_batch_scans(scan_paths):\n    scans = np.zeros((len(scan_paths), IMAGE_RESOLUTION[0], IMAGE_RESOLUTION[1], 3))\n    for i, path in enumerate(scan_paths):\n        s, f = get_img(path)\n        s = rainbow_window(s, f)\n        scans[i] = s\n        \n    return scans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# t0 = time()\n# scans, fields = load_scans(str(TRAIN_ROOT/studies[100]/os.listdir(TRAIN_ROOT/studies[100])[0]))\n# t1 = time()\n# print(t1 - t0)\n# s = scans[index]\n# #print(s)\n# pl.imshow(s, cmap=pl.cm.bone)\n# pl.show()\n\n\n# t0 = time()\n# s = rainbow_window(scans[index], fields[index])\n# t1 = time()\n# print(t1 - t0)\n# #print(s)\n# pl.imshow(s, cmap=pl.cm.bone)\n# pl.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **TFRecords**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TFRecord formats\n\nUNLABELED_TFRECORD_FORMAT = {'SpecificCharacterSet': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'ImageType': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SOPClassUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SOPInstanceUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'Modality': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SliceThickness': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'KVP': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'GantryDetectorTilt': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'TableHeight': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'RotationDirection': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'XRayTubeCurrent': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'Exposure': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'ConvolutionKernel': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'PatientPosition': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'StudyInstanceUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SeriesInstanceUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SeriesNumber': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'InstanceNumber': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'ImagePositionPatient': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'ImageOrientationPatient': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'FrameOfReferenceUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SamplesPerPixel': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'PhotometricInterpretation': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'Rows': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'Columns': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'PixelSpacing': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'BitsAllocated': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'BitsStored': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'HighBit': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'PixelRepresentation': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'WindowCenter': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'WindowWidth': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'RescaleIntercept': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'RescaleSlope': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'image': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None)}\n\nLABELED_TFRECORD_FORMAT = {'SpecificCharacterSet': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'ImageType': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SOPClassUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SOPInstanceUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'Modality': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SliceThickness': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'KVP': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'GantryDetectorTilt': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'TableHeight': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'RotationDirection': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'XRayTubeCurrent': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'Exposure': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'ConvolutionKernel': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'PatientPosition': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'StudyInstanceUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SeriesInstanceUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SeriesNumber': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'InstanceNumber': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'ImagePositionPatient': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'ImageOrientationPatient': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'FrameOfReferenceUID': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'SamplesPerPixel': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'PhotometricInterpretation': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'Rows': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'Columns': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'PixelSpacing': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'BitsAllocated': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'BitsStored': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'HighBit': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'PixelRepresentation': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'WindowCenter': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'WindowWidth': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'RescaleIntercept': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'RescaleSlope': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None),\n 'negative_exam_for_pe': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'qa_motion': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'qa_contrast': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'flow_artifact': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'rv_lv_ratio_gte_1': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'rv_lv_ratio_lt_1': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'leftsided_pe': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'chronic_pe': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'true_filling_defect_not_pe': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'rightsided_pe': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'acute_and_chronic_pe': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'central_pe': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'indeterminate': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'pe_present_on_image': tf.io.FixedLenFeature(shape=(), dtype=tf.int64, default_value=None),\n 'image': tf.io.FixedLenFeature(shape=(), dtype=tf.string, default_value=None)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TFRecord data loaders\n\ndef read_labeled_tfrecord(example):\n    return read_tfrecord(example, LABELED_TFRECORD_FORMAT)\n\ndef read_unlabeled_tfrecord(example):\n    return read_tfrecord(example, UNLABELED_TFRECORD_FORMAT)\n\ndef read_tfrecord(example, record_format):\n    try:\n        example = tf.io.parse_single_example(example, record_format)\n    except:\n        print (example)\n        raise\n    \n    data = {k:tf.cast(example[k], record_format[k].dtype) for k in example}\n        \n    return data\n\ndef load_dataset(filenames, batch_size, repeat=True, labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord).batch(batch_size, drop_remainder=True)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset.repeat() if repeat else dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing function to transform the raw numpy images\n# to scaled, windowed, images.\n\ndef preprocess(data):\n    image = np.array([np.frombuffer(im.numpy(), dtype=np.int16) for im in data['image']]).reshape((batch_size, 512, 512, 1))\n    image = tf.image.resize(image, (IMAGE_RESOLUTION[0], IMAGE_RESOLUTION[1])).numpy()\n    image = np.squeeze(image, -1)\n    \n    out_images = np.zeros((batch_size, IMAGE_RESOLUTION[0], IMAGE_RESOLUTION[1], 3))\n    \n    for i, im in enumerate(image):\n        dcm_fields = (int(float(data['RescaleSlope'].numpy()[i].decode())), int(float(data['RescaleIntercept'].numpy()[i].decode())))\n        out_images[i] = rainbow_window(image[i], dcm_fields)\n\n    label = data['pe_present_on_image'].numpy()\n    label = tf.reshape(label, (batch_size, 1))\n    \n    return out_images, label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Splitting Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_IMAGES = 500000\n\nimage_labels = train_csv[['StudyInstanceUID', 'SeriesInstanceUID', 'SOPInstanceUID', 'pe_present_on_image']]\n\nexam_labels = train_csv[['StudyInstanceUID', 'negative_exam_for_pe', 'rv_lv_ratio_gte_1',\n                         'rv_lv_ratio_lt_1', 'leftsided_pe', 'chronic_pe', 'rightsided_pe',\n                         'acute_and_chronic_pe', 'central_pe', 'indeterminate']].drop_duplicates('StudyInstanceUID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"negative_indices = list(image_labels.loc[image_labels['pe_present_on_image'] == 0].axes[0])\npositive_indices = list(image_labels.loc[image_labels['pe_present_on_image'] == 1].axes[0])\n\nall_indices = negative_indices + positive_indices\n\n# ratio of selected max number of images to the total number of images\n# note that this ratio could be adjusted since the entire dataset is huge\nratio = MAX_IMAGES / len(all_indices)\n\npositive_count = len(positive_indices)#int(ratio * len(positive_indices))\nnegative_count = MAX_IMAGES - positive_count\n\npositive_images = sample(positive_indices, positive_count)\nnegative_images = sample(negative_indices, negative_count)\n\ntraining_indices = positive_images + negative_images\nshuffle(training_indices)\n\nprint(\"Ratio:\", ratio)\nprint(\"Total scans:\", len(all_indices))\nprint(\"Total positive scans:\", len(positive_indices))\nprint(f\"Max images, positve_count: {MAX_IMAGES}, {positive_count}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 24\nepochs = 2\nlearning_rate = 1e-6\nimage_opt = keras.optimizers.Adam(learning_rate=learning_rate, amsgrad=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model definitions\nkeras.backend.clear_session()\n\nimage_inp = keras.Input(shape=(IMAGE_RESOLUTION[0], IMAGE_RESOLUTION[1], 3))\nimage_model = keras.applications.EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')(image_inp)\nimage_model = layers.Dense(1, activation='sigmoid')(image_model)\nimage_model = keras.Model(inputs=image_inp, outputs=image_model)\n\nimage_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# Loss weights to apply to the calculated loss\n# Ratio of all the training samples to the positive training samples\nloss_weights = np.full((batch_size, 1), len(training_indices)/len(positive_images))\n\n# Loss function with the option of weighting classes\n# Uses the defined loss_weights above\ndef loss_function(labels, logits, weighted=True):\n    loss = keras.losses.binary_crossentropy(labels, logits, from_logits=True, label_smoothing=0.05)\n    \n    if weighted:\n        weights = tf.cast(tf.math.greater(labels, 0), tf.float32)*loss_weights\n        weights += tf.cast(tf.math.equal(weights, 0), tf.float32)\n        weights = tf.reshape(weights, (weights.shape[0],))\n        \n        loss = tf.math.multiply(loss, weights)\n\n    loss = tf.math.reduce_mean(loss)\n    \n    return loss\n\n@tf.function\ndef image_train_step(image, labels):\n    with tf.GradientTape() as tape:\n        logits = image_model(image)\n        loss = loss_function(labels, logits, weighted=True)\n        \n    gradients = tape.gradient(loss, image_model.trainable_variables)\n    image_opt.apply_gradients(zip(gradients, image_model.trainable_variables))\n    \n    return loss, logits\n\ndef image_train():\n    loss_met = keras.metrics.Mean()\n    acc_met = keras.metrics.AUC(curve='PR')\n    \n    total_iterations = len(training_indices)//batch_size\n    total_iterations = 2\n    \n    for i in range(total_iterations):\n        ## retrieving scans and their labels\n        t0 = time()\n        indices = training_indices[i*batch_size:(i+1)*batch_size]\n        scan_paths = []\n        \n        # getting labels\n        labels = np.array(image_labels['pe_present_on_image'][pd.Index(indices)]).astype(np.int32)\n        \n        # getting scans\n        study = image_labels['StudyInstanceUID'][indices]\n        series = image_labels['SeriesInstanceUID'][indices]\n        scan = image_labels['SOPInstanceUID'][indices]\n        \n        scan_paths = list(map(lambda study, series, scan: str(TRAIN_ROOT/('/'.join([study, series, scan+\".dcm\"]))) , study, series, scan))\n        scans = load_batch_scans(scan_paths)\n\n        ## train for this iteration\n        t1 = time()\n        loss, logits = image_train_step(scans, tf.reshape(labels, (batch_size, 1)))\n        t2 = time()\n\n        loss_met(loss)\n        acc_met(labels, logits)\n        \n        print(f\"Batch {i} of {total_iterations} loss, accuracy: {loss_met.result()}, {acc_met.result()}      \\r\", end='')\n        \n    return loss_met.result(), acc_met.result()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for e in range(epochs):\n    loss, acc = image_train()\n    print(f\"Epoch {e+1} loss, accuracy: {loss}, {acc}\")\n    image_model.save_weights(\"image_weights.h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Evaluate**"},{"metadata":{},"cell_type":"markdown","source":"Cross validate"},{"metadata":{"trusted":true},"cell_type":"code","source":"# doing k-fold cross validation from the full training dataset to preserve the skewed ratio\nk = 5\nfold_size = 100000\n\n# generating folds\ndef generate_folds(k, fold_size):\n\n    assert fold_size < len(image_labels)/k\n\n    indices = list(range(len(image_labels)))\n    folds = list()\n    \n    for i in range(k):\n        shuffle(indices)\n\n        fold = indices[:fold_size]\n        folds.append(fold)\n\n        indices = indices[fold_size:]\n    \n    return folds\n\n@tf.function\ndef image_eval_step(image, labels):\n    logits = image_model(image)\n    loss = keras.losses.binary_crossentropy(labels, logits)\n\n    return loss, logits\n    \n# running CV\ndef cross_validate(folds):\n    loss_met = keras.metrics.Mean()\n    auc_pr_met = keras.metrics.AUC(curve='PR')\n\n    count = 0\n\n    for fold in folds:\n        total_iterations = len(fold)//batch_size\n        count = count + 1\n        total_iterations = 2\n\n        for i in range(total_iterations):\n\n            indices = fold[i*batch_size:(i+1)*batch_size]\n            scan_paths = []\n\n            # getting labels\n            labels = np.array(image_labels['pe_present_on_image'][pd.Index(indices)]).astype(np.int32)\n\n            # getting scans\n            study = image_labels['StudyInstanceUID'][indices]\n            series = image_labels['SeriesInstanceUID'][indices]\n            scan = image_labels['SOPInstanceUID'][indices]\n\n            scan_paths = list(map(lambda study, series, scan: str(TRAIN_ROOT/('/'.join([study, series, scan+\".dcm\"]))) , study, series, scan))\n            scans = load_batch_scans(scan_paths)\n\n            ## feed forward\n            t1 = time()\n            loss, logits = image_train_step(scans, tf.reshape(labels, (batch_size, 1)))\n            t2 = time()\n\n            loss_met(loss)\n            auc_pr_met(labels, logits)\n\n        print('Fold {} of {} folds, loss: {}, auc_pr: {}, time: {}'.format(count, len(folds), loss_met.result(), auc_pr_met.result(), t2-t1))\n\nfolds = generate_folds(k, fold_size)\ncross_validate(folds)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}